"""
ë§ˆìŠ¤í† ëˆ ì˜ˆì•½ ë´‡ ìºì‹œ ê´€ë¦¬ ì‹œìŠ¤í…œ
ì‹œíŠ¸ ë°ì´í„°ì˜ ë³€ê²½ ê°ì§€, JSON ìºì‹œ ê´€ë¦¬, ë°±ì—… ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import hashlib
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from pathlib import Path
import pytz

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(current_dir)

try:
    from config.settings import config
    from utils.logging_config import get_logger, log_performance, LogContext
    from utils.datetime_utils import format_datetime_korean, default_parser
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

logger = get_logger(__name__)


class CacheEntry:
    """
    ìºì‹œ ì—”íŠ¸ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤
    ê°œë³„ íˆ¿ ë°ì´í„°ì˜ ìºì‹œ ì •ë³´ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, row_index: int, date_str: str, time_str: str, 
                 account: str, content: str, content_hash: str, 
                 scheduled_datetime: Optional[datetime] = None,
                 status: str = 'pending'):
        """
        CacheEntry ì´ˆê¸°í™”
        
        Args:
            row_index: ì‹œíŠ¸ì—ì„œì˜ í–‰ ë²ˆí˜¸
            date_str: ë‚ ì§œ ë¬¸ìì—´ (ì›ë³¸)
            time_str: ì‹œê°„ ë¬¸ìì—´ (ì›ë³¸)
            account: ê³„ì • ì´ë¦„
            content: íˆ¿ ë‚´ìš©
            content_hash: ë‚´ìš© í•´ì‹œê°’
            scheduled_datetime: íŒŒì‹±ëœ ì˜ˆì•½ ì‹œê°„
            status: ìƒíƒœ ('pending', 'posted', 'failed', 'skipped')
        """
        self.row_index = row_index
        self.date_str = date_str
        self.time_str = time_str
        self.account = account
        self.content = content
        self.content_hash = content_hash
        self.scheduled_datetime = scheduled_datetime
        self.status = status
        self.created_at = datetime.now(pytz.timezone('Asia/Seoul'))
        self.updated_at = self.created_at
        self.posted_at = None
        self.error_message = None
        self.retry_count = 0
    
    @classmethod
    def from_toot_data(cls, toot_data) -> 'CacheEntry':
        """
        TootData ê°ì²´ë¡œë¶€í„° CacheEntry ìƒì„±
        
        Args:
            toot_data: TootData ê°ì²´
        
        Returns:
            CacheEntry: ìƒì„±ëœ ìºì‹œ ì—”íŠ¸ë¦¬
        """
        content_hash = cls.calculate_content_hash(
            toot_data.date_str, 
            toot_data.time_str, 
            toot_data.account,
            toot_data.content
        )
        
        return cls(
            row_index=toot_data.row_index,
            date_str=toot_data.date_str,
            time_str=toot_data.time_str,
            account=toot_data.account,
            content=toot_data.content,
            content_hash=content_hash,
            scheduled_datetime=toot_data.scheduled_datetime
        )
    
    @staticmethod
    def calculate_content_hash(date_str: str, time_str: str, account: str, content: str) -> str:
        """
        ë‚´ìš© í•´ì‹œê°’ ê³„ì‚°
        
        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´
            time_str: ì‹œê°„ ë¬¸ìì—´
            account: ê³„ì • ì´ë¦„
            content: íˆ¿ ë‚´ìš©
        
        Returns:
            str: SHA256 í•´ì‹œê°’
        """
        combined = f"{date_str}|{time_str}|{account}|{content}"
        return hashlib.sha256(combined.encode('utf-8')).hexdigest()[:16]
    
    def update_status(self, status: str, error_message: Optional[str] = None) -> None:
        """
        ìƒíƒœ ì—…ë°ì´íŠ¸
        
        Args:
            status: ìƒˆë¡œìš´ ìƒíƒœ
            error_message: ì˜¤ë¥˜ ë©”ì‹œì§€ (ìˆëŠ” ê²½ìš°)
        """
        self.status = status
        self.error_message = error_message
        self.updated_at = datetime.now(pytz.timezone('Asia/Seoul'))
        
        if status == 'posted':
            self.posted_at = self.updated_at
        elif status == 'failed':
            self.retry_count += 1
    
    def is_expired(self, current_time: Optional[datetime] = None) -> bool:
        """
        ìºì‹œ ì—”íŠ¸ë¦¬ê°€ ë§Œë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
        
        Args:
            current_time: í˜„ì¬ ì‹œê°„ (Noneì´ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©)
        
        Returns:
            bool: ë§Œë£Œ ì—¬ë¶€
        """
        if current_time is None:
            current_time = default_parser.get_current_datetime()
        
        # ì´ë¯¸ í¬ìŠ¤íŒ…ëœ ê²ƒì€ ë§Œë£Œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
        if self.status == 'posted':
            return True
        
        # ì˜ˆì•½ ì‹œê°„ì´ ê³¼ê±°ë©´ ë§Œë£Œ
        if self.scheduled_datetime and self.scheduled_datetime < current_time:
            return True
        
        return False
    
    def can_retry(self, max_retries: int = 3) -> bool:
        """
        ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        
        Args:
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
        Returns:
            bool: ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€
        """
        return self.status == 'failed' and self.retry_count < max_retries
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'row_index': self.row_index,
            'date_str': self.date_str,
            'time_str': self.time_str,
            'account': self.account,
            'content': self.content,
            'content_hash': self.content_hash,
            'scheduled_datetime': self.scheduled_datetime.isoformat() if self.scheduled_datetime else None,
            'status': self.status,
            'created_at': self.created_at.isoformat(),
            'updated_at': self.updated_at.isoformat(),
            'posted_at': self.posted_at.isoformat() if self.posted_at else None,
            'error_message': self.error_message,
            'retry_count': self.retry_count
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CacheEntry':
        """ë”•ì…”ë„ˆë¦¬ë¡œë¶€í„° ê°ì²´ ìƒì„±"""
        entry = cls(
            row_index=data['row_index'],
            date_str=data['date_str'],
            time_str=data['time_str'],
            account=data.get('account', ''),  # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ë³¸ê°’ ì œê³µ
            content=data['content'],
            content_hash=data['content_hash'],
            status=data.get('status', 'pending')
        )
        
        # ì‹œê°„ ì •ë³´ ë³µì›
        if data.get('scheduled_datetime'):
            entry.scheduled_datetime = datetime.fromisoformat(data['scheduled_datetime'])
        
        if data.get('created_at'):
            entry.created_at = datetime.fromisoformat(data['created_at'])
        
        if data.get('updated_at'):
            entry.updated_at = datetime.fromisoformat(data['updated_at'])
        
        if data.get('posted_at'):
            entry.posted_at = datetime.fromisoformat(data['posted_at'])
        
        entry.error_message = data.get('error_message')
        entry.retry_count = data.get('retry_count', 0)
        
        return entry
    
    def get_cache_key(self) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (í–‰ë²ˆí˜¸ + í•´ì‹œ)"""
        return f"row_{self.row_index}_{self.content_hash}"
    
    def __str__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        time_str = format_datetime_korean(self.scheduled_datetime) if self.scheduled_datetime else "ì‹œê°„ ë¯¸ì •"
        return f"[í–‰{self.row_index}] {self.account} | {time_str} | {self.status} | {self.content[:30]}..."


def format_time_until(target_time: Optional[datetime], current_time: Optional[datetime] = None) -> str:
    """
    ë‘ ì‹œê°„ ì‚¬ì´ì˜ ë‚¨ì€ ì‹œê°„ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if target_time is None:
        return "ì‹œê°„ ë¯¸ì •"
    if current_time is None:
        current_time = datetime.now(pytz.timezone('Asia/Seoul'))
    delta = target_time - current_time
    if delta.total_seconds() < 0:
        return "ì§€ë‚¨"
    days = delta.days
    hours, remainder = divmod(delta.seconds, 3600)
    minutes, _ = divmod(remainder, 60)
    parts = []
    if days > 0:
        parts.append(f"{days}ì¼")
    if hours > 0:
        parts.append(f"{hours}ì‹œê°„")
    if minutes > 0:
        parts.append(f"{minutes}ë¶„")
    if not parts:
        return "ê³§"
    return " ".join(parts) + " ë‚¨ìŒ"

class CacheManager:
    """
    ìºì‹œ ê´€ë¦¬ ì‹œìŠ¤í…œ
    ì‹œíŠ¸ ë°ì´í„°ì˜ ë³€ê²½ ê°ì§€ ë° JSON ìºì‹œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, cache_file_path: Optional[Path] = None,
                 backup_dir_path: Optional[Path] = None):
        """
        CacheManager ì´ˆê¸°í™”
        
        Args:
            cache_file_path: ìºì‹œ íŒŒì¼ ê²½ë¡œ
            backup_dir_path: ë°±ì—… ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        # ê²½ë¡œ ì„¤ì •
        self.cache_file_path = cache_file_path or config.get_cache_file_path()
        self.backup_dir_path = backup_dir_path or config.get_backup_dir_path()
        
        # ìºì‹œ ë°ì´í„°
        self.cache_entries: Dict[str, CacheEntry] = {}
        self.metadata = {
            'version': '1.0',
            'created_at': None,
            'last_updated': None,
            'last_sync_time': None,
            'sync_count': 0,
            'total_entries': 0,
            'pending_entries': 0,
            'posted_entries': 0,
            'failed_entries': 0
        }
        
        # ì„¤ì •
        self.backup_retention_days = getattr(config, 'CACHE_BACKUP_RETENTION_DAYS', 30)
        self.auto_cleanup_enabled = getattr(config, 'CACHE_AUTO_CLEANUP', True)
        self.max_retry_attempts = getattr(config, 'MAX_RETRY_ATTEMPTS', 3)
        
        # í†µê³„
        self.stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'cache_updates': 0,
            'backup_count': 0,
            'cleanup_count': 0,
            'load_count': 0,
            'save_count': 0
        }
        
        logger.info(f"ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”: {self.cache_file_path}")
        
        # ì´ˆê¸° ë¡œë“œ
        self.load_cache()
    
    @log_performance
    def load_cache(self) -> bool:
        """
        ìºì‹œ íŒŒì¼ ë¡œë“œ
        
        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not self.cache_file_path.exists():
                logger.info("ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                self._initialize_empty_cache()
                return True
            
            with open(self.cache_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            self.metadata.update(data.get('metadata', {}))
            
            # ìºì‹œ ì—”íŠ¸ë¦¬ ë¡œë“œ
            entries_data = data.get('entries', {})
            self.cache_entries = {}
            
            for key, entry_data in entries_data.items():
                try:
                    entry = CacheEntry.from_dict(entry_data)
                    self.cache_entries[key] = entry
                except Exception as e:
                    logger.warning(f"ìºì‹œ ì—”íŠ¸ë¦¬ ë¡œë“œ ì‹¤íŒ¨ (í‚¤: {key}): {e}")
            
            self._update_metadata_stats()
            self.stats['load_count'] += 1
            
            logger.info(f"ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.cache_entries)}ê°œ ì—”íŠ¸ë¦¬")
            return True
            
        except json.JSONDecodeError as e:
            logger.error(f"ìºì‹œ íŒŒì¼ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return self._handle_corrupted_cache()
        except Exception as e:
            logger.error(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._handle_corrupted_cache()
    
    def _handle_corrupted_cache(self) -> bool:
        """ì†ìƒëœ ìºì‹œ íŒŒì¼ ì²˜ë¦¬"""
        logger.warning("ì†ìƒëœ ìºì‹œ íŒŒì¼ì„ ë°±ì—…í•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        try:
            # ì†ìƒëœ íŒŒì¼ ë°±ì—…
            if self.cache_file_path.exists():
                corrupt_backup = self.backup_dir_path / f"corrupted_cache_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                shutil.copy2(self.cache_file_path, corrupt_backup)
                logger.info(f"ì†ìƒëœ ìºì‹œ íŒŒì¼ ë°±ì—…: {corrupt_backup}")
            
            # ìƒˆë¡œìš´ ìºì‹œ ì´ˆê¸°í™”
            self._initialize_empty_cache()
            return True
            
        except Exception as e:
            logger.error(f"ì†ìƒëœ ìºì‹œ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _initialize_empty_cache(self) -> None:
        """ë¹ˆ ìºì‹œ ì´ˆê¸°í™”"""
        self.cache_entries = {}
        self.metadata = {
            'version': '1.0',
            'created_at': datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
            'last_updated': None,
            'last_sync_time': None,
            'sync_count': 0,
            'total_entries': 0,
            'pending_entries': 0,
            'posted_entries': 0,
            'failed_entries': 0
        }
        self.save_cache()

    def clear_cache(self) -> bool:
        """
        ìºì‹œ íŒŒì¼ê³¼ ë°±ì—… ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        """
        try:
            if self.cache_file_path.exists():
                os.remove(self.cache_file_path)
                logger.info(f"âœ… ìºì‹œ íŒŒì¼ ì‚­ì œ: {self.cache_file_path}")
            
            # ë°±ì—… ë””ë ‰í† ë¦¬ë„ ì •ë¦¬í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
            # if self.backup_dir_path.exists() and self.backup_dir_path.is_dir():
            #     shutil.rmtree(self.backup_dir_path)
            #     logger.info(f"âœ… ë°±ì—… ë””ë ‰í† ë¦¬ ì‚­ì œ: {self.backup_dir_path}")

            self._initialize_empty_cache() # ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ìƒíƒœë„ ì •ë¦¬
            return True
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    @log_performance
    def save_cache(self, create_backup: bool = True) -> bool:
        """
        ìºì‹œ íŒŒì¼ ì €ì¥
        
        Args:
            create_backup: ë°±ì—… ìƒì„± ì—¬ë¶€
        
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with LogContext("ìºì‹œ ì €ì¥") as ctx:
                
                if create_backup and self.cache_file_path.exists():
                    ctx.log_step("ê¸°ì¡´ ìºì‹œ ë°±ì—…")
                    self._create_backup()
                
                ctx.log_step("ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸")
                self._update_metadata_stats()
                self.metadata['last_updated'] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                
                ctx.log_step("JSON ë°ì´í„° ì¤€ë¹„")
                # ìºì‹œ ì—”íŠ¸ë¦¬ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                entries_data = {}
                for key, entry in self.cache_entries.items():
                    entries_data[key] = entry.to_dict()
                
                # ì „ì²´ ë°ì´í„° êµ¬ì„±
                cache_data = {
                    'metadata': self.metadata,
                    'entries': entries_data
                }
                
                ctx.log_step("íŒŒì¼ ì“°ê¸°")
                # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì“°ê³  ì›ìì ìœ¼ë¡œ ì´ë™
                temp_file = self.cache_file_path.with_suffix('.tmp')
                
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
                # ì›ìì  ì´ë™
                temp_file.replace(self.cache_file_path)
                
                self.stats['save_count'] += 1
                logger.debug(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.cache_entries)}ê°œ ì—”íŠ¸ë¦¬")
                return True
                
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _create_backup(self) -> bool:
        """ìºì‹œ ë°±ì—… ìƒì„±"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_file = self.backup_dir_path / f"cache_backup_{timestamp}.json"
            
            shutil.copy2(self.cache_file_path, backup_file)
            self.stats['backup_count'] += 1
            
            logger.debug(f"ìºì‹œ ë°±ì—… ìƒì„±: {backup_file}")
            
            # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
            if self.auto_cleanup_enabled:
                self._cleanup_old_backups()
            
            return True
            
        except Exception as e:
            logger.error(f"ìºì‹œ ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def _cleanup_old_backups(self) -> None:
        """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.backup_retention_days)
            
            for backup_file in self.backup_dir_path.glob("cache_backup_*.json"):
                if backup_file.stat().st_mtime < cutoff_date.timestamp():
                    backup_file.unlink()
                    self.stats['cleanup_count'] += 1
                    logger.debug(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {backup_file}")
                    
        except Exception as e:
            logger.error(f"ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def sync_with_sheet_data(self, toot_data_list: List) -> Tuple[bool, Dict[str, Any]]:
        """
        ì‹œíŠ¸ ë°ì´í„°ì™€ ìºì‹œ ë™ê¸°í™”
        
        Args:
            toot_data_list: ì‹œíŠ¸ì—ì„œ ì¡°íšŒí•œ TootData ëª©ë¡
        
        Returns:
            Tuple[bool, Dict[str, Any]]: (ë³€ê²½ ì—¬ë¶€, ë³€ê²½ í†µê³„)
        """
        with LogContext("ìºì‹œ ë™ê¸°í™”") as ctx:
            ctx.log_step("ë³€ê²½ ê°ì§€ ì‹œì‘")
            
            # í˜„ì¬ ì‹œíŠ¸ ë°ì´í„°ë¥¼ ìºì‹œ ì—”íŠ¸ë¦¬ë¡œ ë³€í™˜
            new_entries = {}
            for toot_data in toot_data_list:
                if toot_data.is_valid:  # ìœ íš¨í•œ ë°ì´í„°ë§Œ ìºì‹œ
                    entry = CacheEntry.from_toot_data(toot_data)
                    cache_key = entry.get_cache_key()
                    new_entries[cache_key] = entry
            
            ctx.log_step("ë³€ê²½ì‚¬í•­ ë¶„ì„")
            
            # ë³€ê²½ í†µê³„
            changes = {
                'added': [],      # ìƒˆë¡œ ì¶”ê°€ëœ ì—”íŠ¸ë¦¬
                'updated': [],    # ë‚´ìš©ì´ ë³€ê²½ëœ ì—”íŠ¸ë¦¬
                'removed': [],    # ì œê±°ëœ ì—”íŠ¸ë¦¬
                'unchanged': []   # ë³€ê²½ë˜ì§€ ì•Šì€ ì—”íŠ¸ë¦¬
            }
            
            # ê¸°ì¡´ ìºì‹œì™€ ë¹„êµ
            existing_keys = set(self.cache_entries.keys())
            new_keys = set(new_entries.keys())
            
            # ìƒˆë¡œ ì¶”ê°€ëœ ì—”íŠ¸ë¦¬
            for key in new_keys - existing_keys:
                new_entry = new_entries[key]
                self.cache_entries[key] = new_entry
                changes['added'].append(new_entry)
                self.stats['cache_misses'] += 1
            
            # ì œê±°ëœ ì—”íŠ¸ë¦¬ (ë” ì´ìƒ ì‹œíŠ¸ì— ì—†ìŒ)
            for key in existing_keys - new_keys:
                removed_entry = self.cache_entries[key]
                # ì•„ì§ í¬ìŠ¤íŒ…ë˜ì§€ ì•Šì€ ê²ƒë§Œ ì œê±° (í¬ìŠ¤íŒ…ëœ ê²ƒì€ ê¸°ë¡ ë³´ì¡´)
                if removed_entry.status != 'posted':
                    changes['removed'].append(removed_entry)
                    del self.cache_entries[key]
            
            # ê³µí†µ ì—”íŠ¸ë¦¬ - ë‚´ìš© ë³€ê²½ í™•ì¸
            for key in existing_keys & new_keys:
                existing_entry = self.cache_entries[key]
                new_entry = new_entries[key]
                
                # ë‚´ìš© í•´ì‹œ ë¹„êµ
                if existing_entry.content_hash != new_entry.content_hash:
                    # ë‚´ìš©ì´ ë³€ê²½ë¨ - ê¸°ì¡´ ìƒíƒœëŠ” ìœ ì§€í•˜ë˜ ë‚´ìš©ë§Œ ì—…ë°ì´íŠ¸
                    existing_entry.content = new_entry.content
                    existing_entry.content_hash = new_entry.content_hash
                    existing_entry.date_str = new_entry.date_str
                    existing_entry.time_str = new_entry.time_str
                    existing_entry.scheduled_datetime = new_entry.scheduled_datetime
                    existing_entry.updated_at = datetime.now(pytz.timezone('Asia/Seoul'))
                    
                    # ì´ë¯¸ í¬ìŠ¤íŒ…ëœ ê²ƒì´ ë³€ê²½ë˜ë©´ ê²½ê³ 
                    if existing_entry.status == 'posted':
                        logger.warning(f"ì´ë¯¸ í¬ìŠ¤íŒ…ëœ íˆ¿ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤: í–‰ {existing_entry.row_index}")
                    
                    changes['updated'].append(existing_entry)
                    self.stats['cache_updates'] += 1
                else:
                    changes['unchanged'].append(existing_entry)
                    self.stats['cache_hits'] += 1
            
            ctx.log_step("ë™ê¸°í™” ì™„ë£Œ")
            
            # ë³€ê²½ì´ ìˆì—ˆëŠ”ì§€ í™•ì¸
            has_changes = bool(changes['added'] or changes['updated'] or changes['removed'])
            
            if has_changes:
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                self.metadata['last_sync_time'] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                self.metadata['sync_count'] += 1
                
                # ìºì‹œ ì €ì¥
                self.save_cache()
                
                logger.info(f"ìºì‹œ ë™ê¸°í™” ì™„ë£Œ: ì¶”ê°€ {len(changes['added'])}ê°œ, "
                           f"ìˆ˜ì • {len(changes['updated'])}ê°œ, ì‚­ì œ {len(changes['removed'])}ê°œ")
            else:
                logger.debug("ìºì‹œ ë™ê¸°í™”: ë³€ê²½ì‚¬í•­ ì—†ìŒ")
            
            return has_changes, changes
    
    def get_pending_entries(self, current_time: Optional[datetime] = None) -> List[CacheEntry]:
        """
        ëŒ€ê¸° ì¤‘ì¸ ì—”íŠ¸ë¦¬ ëª©ë¡ ë°˜í™˜
        
        Args:
            current_time: í˜„ì¬ ì‹œê°„ (Noneì´ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©)
        
        Returns:
            List[CacheEntry]: ëŒ€ê¸° ì¤‘ì¸ ì—”íŠ¸ë¦¬ ëª©ë¡ (ì‹œê°„ìˆœ ì •ë ¬)
        """
        if current_time is None:
            current_time = default_parser.get_current_datetime()
        
        pending_entries = [
            entry for entry in self.cache_entries.values()
            if entry.status == 'pending' and 
               entry.scheduled_datetime and 
               entry.scheduled_datetime > current_time
        ]
        
        # ì˜ˆì•½ ì‹œê°„ìˆœ ì •ë ¬
        pending_entries.sort(key=lambda e: e.scheduled_datetime)
        
        return pending_entries
    
    def get_due_entries(self, current_time: Optional[datetime] = None,
                       buffer_minutes: int = 1) -> List[CacheEntry]:
        """
        ì‹¤í–‰ ì‹œê°„ì´ ëœ ì—”íŠ¸ë¦¬ ëª©ë¡ ë°˜í™˜
        
        Args:
            current_time: í˜„ì¬ ì‹œê°„
            buffer_minutes: ë²„í¼ ì‹œê°„ (ë¶„) - ì´ ì‹œê°„ë§Œí¼ ì¼ì° ì‹¤í–‰
        
        Returns:
            List[CacheEntry]: ì‹¤í–‰í•  ì—”íŠ¸ë¦¬ ëª©ë¡
        """
        if current_time is None:
            current_time = default_parser.get_current_datetime()
        
        # ë²„í¼ ì‹œê°„ ì ìš©
        execution_time = current_time + timedelta(minutes=buffer_minutes)
        
        due_entries = [
            entry for entry in self.cache_entries.values()
            if entry.status == 'pending' and 
               entry.scheduled_datetime and 
               entry.scheduled_datetime <= execution_time
        ]
        
        # ì˜ˆì•½ ì‹œê°„ìˆœ ì •ë ¬
        due_entries.sort(key=lambda e: e.scheduled_datetime)
        
        return due_entries
    
    def get_retry_candidates(self, current_time: Optional[datetime] = None,
                            retry_delay_minutes: int = 30) -> List[CacheEntry]:
        """
        ì¬ì‹œë„ ê°€ëŠ¥í•œ ì‹¤íŒ¨ ì—”íŠ¸ë¦¬ ëª©ë¡ ë°˜í™˜
        
        Args:
            current_time: í˜„ì¬ ì‹œê°„
            retry_delay_minutes: ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ë¶„)
        
        Returns:
            List[CacheEntry]: ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—”íŠ¸ë¦¬ ëª©ë¡
        """
        if current_time is None:
            current_time = default_parser.get_current_datetime()
        
        retry_cutoff = current_time - timedelta(minutes=retry_delay_minutes)
        
        retry_entries = [
            entry for entry in self.cache_entries.values()
            if entry.can_retry(self.max_retry_attempts) and
               entry.updated_at < retry_cutoff
        ]
        
        # ì—…ë°ì´íŠ¸ ì‹œê°„ìˆœ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
        retry_entries.sort(key=lambda e: e.updated_at)
        
        return retry_entries
    
    def update_entry_status(self, cache_key: str, status: str, 
                           error_message: Optional[str] = None) -> bool:
        """
        ì—”íŠ¸ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        
        Args:
            cache_key: ìºì‹œ í‚¤
            status: ìƒˆë¡œìš´ ìƒíƒœ
            error_message: ì˜¤ë¥˜ ë©”ì‹œì§€ (ìˆëŠ” ê²½ìš°)
        
        Returns:
            bool: ì—…ë°ì´íŠ¸ ì„±ê³µ ì—¬ë¶€
        """
        if cache_key not in self.cache_entries:
            logger.error(f"ìºì‹œ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cache_key}")
            return False
        
        entry = self.cache_entries[cache_key]
        old_status = entry.status
        
        entry.update_status(status, error_message)
        
        logger.debug(f"ì—”íŠ¸ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸: {cache_key} | {old_status} -> {status}")
        
        # ì£¼ìš” ìƒíƒœ ë³€ê²½ì€ ì¦‰ì‹œ ì €ì¥
        if status in ['posted', 'failed']:
            self.save_cache(create_backup=False)
        
        return True
    
    def cleanup_expired_entries(self, current_time: Optional[datetime] = None) -> int:
        """
        ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì •ë¦¬
        
        Args:
            current_time: í˜„ì¬ ì‹œê°„
        
        Returns:
            int: ì •ë¦¬ëœ ì—”íŠ¸ë¦¬ ìˆ˜
        """
        if current_time is None:
            current_time = default_parser.get_current_datetime()
        
        expired_keys = []
        
        for key, entry in self.cache_entries.items():
            if entry.is_expired(current_time):
                expired_keys.append(key)
        
        # ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì œê±°
        for key in expired_keys:
            del self.cache_entries[key]
        
        if expired_keys:
            self.save_cache(create_backup=False)
            logger.info(f"ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ {len(expired_keys)}ê°œ ì •ë¦¬ ì™„ë£Œ")
        
        return len(expired_keys)
    
    def _update_metadata_stats(self) -> None:
        """ë©”íƒ€ë°ì´í„° í†µê³„ ì—…ë°ì´íŠ¸"""
        status_counts = {'pending': 0, 'posted': 0, 'failed': 0, 'skipped': 0}
        
        for entry in self.cache_entries.values():
            status = entry.status
            if status in status_counts:
                status_counts[status] += 1
        
        self.metadata['total_entries'] = len(self.cache_entries)
        self.metadata['pending_entries'] = status_counts['pending']
        self.metadata['posted_entries'] = status_counts['posted']
        self.metadata['failed_entries'] = status_counts['failed']
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        current_time = default_parser.get_current_datetime()
        
        # ê¸°ë³¸ í†µê³„
        stats = self.stats.copy()
        stats.update(self.metadata)
        
        # ì‹¤ì‹œê°„ í†µê³„
        pending_entries = self.get_pending_entries(current_time)
        due_entries = self.get_due_entries(current_time)
        retry_entries = self.get_retry_candidates(current_time)
        
        stats.update({
            'current_pending': len(pending_entries),
            'current_due': len(due_entries),
            'current_retry_candidates': len(retry_entries),
            'cache_hit_rate': (stats['cache_hits'] / max(stats['cache_hits'] + stats['cache_misses'], 1)) * 100,
            'cache_file_size': self.cache_file_path.stat().st_size if self.cache_file_path.exists() else 0,
            'backup_count': len(list(self.backup_dir_path.glob("cache_backup_*.json")))
        })
        
        return stats
    
    def export_cache_summary(self) -> Dict[str, Any]:
        """ìºì‹œ ìš”ì•½ ì •ë³´ ë‚´ë³´ë‚´ê¸° (ë””ë²„ê¹…ìš©)"""
        summary = {
            'metadata': self.metadata,
            'statistics': self.get_cache_stats(),
            'entries_by_status': {},
            'upcoming_entries': [],
            'recent_failures': []
        }
        
        # ìƒíƒœë³„ ì—”íŠ¸ë¦¬ ê·¸ë£¹í™”
        for status in ['pending', 'posted', 'failed', 'skipped']:
            entries_with_status = [
                {
                    'row_index': entry.row_index,
                    'scheduled_datetime': entry.scheduled_datetime.isoformat() if entry.scheduled_datetime else None,
                    'content_preview': entry.content[:50] + '...' if len(entry.content) > 50 else entry.content,
                    'updated_at': entry.updated_at.isoformat(),
                    'retry_count': entry.retry_count,
                    'error_message': entry.error_message
                }
                for entry in self.cache_entries.values()
                if entry.status == status
            ]
            summary['entries_by_status'][status] = entries_with_status
        
        # ë‹¤ê°€ì˜¤ëŠ” ì˜ˆì•½ (í–¥í›„ 24ì‹œê°„)
        current_time = default_parser.get_current_datetime()
        next_24h = current_time + timedelta(hours=24)
        
        upcoming = [
            {
                'row_index': entry.row_index,
                'scheduled_datetime': entry.scheduled_datetime.isoformat(),
                'content_preview': entry.content[:50] + '...' if len(entry.content) > 50 else entry.content,
                'time_until': format_time_until(entry.scheduled_datetime, current_time)
            }
            for entry in self.cache_entries.values()
            if (entry.status == 'pending' and 
                entry.scheduled_datetime and 
                current_time < entry.scheduled_datetime <= next_24h)
        ]
        upcoming.sort(key=lambda x: x['scheduled_datetime'])
        summary['upcoming_entries'] = upcoming[:10]  # ìµœëŒ€ 10ê°œ
        
        # ìµœê·¼ ì‹¤íŒ¨ (ì§€ë‚œ 24ì‹œê°„)
        past_24h = current_time - timedelta(hours=24)
        
        recent_failures = [
            {
                'row_index': entry.row_index,
                'scheduled_datetime': entry.scheduled_datetime.isoformat() if entry.scheduled_datetime else None,
                'content_preview': entry.content[:50] + '...' if len(entry.content) > 50 else entry.content,
                'error_message': entry.error_message,
                'retry_count': entry.retry_count,
                'updated_at': entry.updated_at.isoformat()
            }
            for entry in self.cache_entries.values()
            if (entry.status == 'failed' and 
                entry.updated_at >= past_24h)
        ]
        recent_failures.sort(key=lambda x: x['updated_at'], reverse=True)
        summary['recent_failures'] = recent_failures[:10]  # ìµœëŒ€ 10ê°œ
        
        return summary
    
    def __str__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        return f"CacheManager({len(self.cache_entries)} entries, {self.cache_file_path})"


# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_cache_manager: Optional[CacheManager] = None


def get_cache_manager() -> CacheManager:
    """ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ë°˜í™˜"""
    global _cache_manager
    
    if _cache_manager is None:
        _cache_manager = CacheManager()
    
    return _cache_manager


def test_cache_system() -> bool:
    """ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ìš© ìºì‹œ ë§¤ë‹ˆì € ìƒì„±
        import tempfile
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_cache = Path(temp_dir) / "test_cache.json"
            temp_backup = Path(temp_dir) / "backup"
            temp_backup.mkdir()
            
            cache_mgr = CacheManager(temp_cache, temp_backup)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            from datetime import datetime
            import pytz
            
            test_entries = []
            for i in range(3):
                entry = CacheEntry(
                    row_index=i + 2,
                    date_str="ë‚´ì¼",
                    time_str=f"{14 + i}:00",
                    content=f"í…ŒìŠ¤íŠ¸ íˆ¿ {i + 1}ë²ˆì…ë‹ˆë‹¤.",
                    content_hash=CacheEntry.calculate_content_hash("ë‚´ì¼", f"{14 + i}:00", f"í…ŒìŠ¤íŠ¸ íˆ¿ {i + 1}ë²ˆì…ë‹ˆë‹¤."),
                    scheduled_datetime=datetime.now(pytz.timezone('Asia/Seoul')) + timedelta(hours=i + 1)
                )
                test_entries.append(entry)
                cache_mgr.cache_entries[entry.get_cache_key()] = entry
            
            # ì €ì¥ í…ŒìŠ¤íŠ¸
            if not cache_mgr.save_cache():
                logger.error("ìºì‹œ ì €ì¥ ì‹¤íŒ¨")
                return False
            
            # ë¡œë“œ í…ŒìŠ¤íŠ¸
            cache_mgr2 = CacheManager(temp_cache, temp_backup)
            if len(cache_mgr2.cache_entries) != 3:
                logger.error("ìºì‹œ ë¡œë“œ ì‹¤íŒ¨")
                return False
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
            first_key = list(cache_mgr2.cache_entries.keys())[0]
            cache_mgr2.update_entry_status(first_key, 'posted')
            
            # í†µê³„ í…ŒìŠ¤íŠ¸
            stats = cache_mgr2.get_cache_stats()
            logger.info(f"í…ŒìŠ¤íŠ¸ í†µê³„: {stats['total_entries']}ê°œ ì—”íŠ¸ë¦¬")
            
            logger.info("âœ… ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            return True
            
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    """ìºì‹œ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ìºì‹œ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        cache_manager = CacheManager()
        
        # ìºì‹œ ë¡œë“œ í…ŒìŠ¤íŠ¸
        print("ğŸ“ ìºì‹œ ë¡œë“œ í…ŒìŠ¤íŠ¸...")
        if cache_manager.load_cache():
            print("âœ… ìºì‹œ ë¡œë“œ ì„±ê³µ")
        else:
            print("âŒ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨")
        
        # í†µê³„ ì •ë³´
        print("ğŸ“Š ìºì‹œ í†µê³„:")
        stats = cache_manager.get_cache_stats()
        print(f"  ì´ ì—”íŠ¸ë¦¬: {stats['total_entries']}ê°œ")
        print(f"  ëŒ€ê¸°ì¤‘: {stats['pending_entries']}ê°œ")
        print(f"  ì™„ë£Œ: {stats['posted_entries']}ê°œ")
        print(f"  ì‹¤íŒ¨: {stats['failed_entries']}ê°œ")
        print(f"  ìºì‹œ ì ì¤‘ë¥ : {stats.get('cache_hit_rate', 0):.1f}%")
        
        # ìºì‹œ ìš”ì•½
        print("ğŸ“‹ ìºì‹œ ìš”ì•½:")
        summary = cache_manager.export_cache_summary()
        
        for status, entries in summary['entries_by_status'].items():
            if entries:
                print(f"  {status}: {len(entries)}ê°œ")
                for entry in entries[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                    print(f"    - í–‰{entry['row_index']}: {entry['content_preview']}")
        
        if summary['upcoming_entries']:
            print("  ë‹¤ê°€ì˜¤ëŠ” ì˜ˆì•½:")
            for entry in summary['upcoming_entries'][:3]:
                print(f"    - {entry['time_until']}: {entry['content_preview']}")
        
        # ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        print("ğŸ”§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
        if test_cache_system():
            print("âœ… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        else:
            print("âŒ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        print("âœ… ìºì‹œ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)